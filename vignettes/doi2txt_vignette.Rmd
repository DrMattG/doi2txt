---
title: "Using doi2txt to extract sections of full text articles"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using doi2txt to extract sections of full text articles}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Read in data

```{r setup}
library(doi2txt)
# Example DOIs used for function testing
# Should all be open access, if I recall correctly

dois <- c("10.1590/S1984-46702014000400006",
          "10.1371/journal.pone.0105397",
          "10.1016/j.cja.2014.04.019",
          "10.11609/JoTT.o3999.7870-8",
          "10.3389/fpsyg.2013.00447",
          "10.5751/ES-02760-140128",
          "10.3389/fevo.2018.00039",
          "10.1515/eje-2017-0011",
          "10.1515/orhu-2015-0017",
          "10.1002/ece3.5395",
          "10.5751/ES-00018-010106",
          "10.1002/ecs2.1351")

# some of these URLs seem to be causing problems
# running through just the first three for testing

# it would be good if we can write some wrapper functions that automatically turn everything into lapply
# that would make everything cleaner looking, though is not technically tidy (I think?)
# I don't think we can do datasets though, since none of the sections will be the same length
# though eventually we can merge paragraphs within sections for final output
# or, I suppose we could do that at this stage and just leave manual line breaks

articles <- lapply(dois[1:3], doi2html)

#sections <- detect_sections(articles[[1]])

methods <- extract_section(articles[[1]], "methods")



```

# 
